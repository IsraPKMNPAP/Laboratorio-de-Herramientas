{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac9155b9f5e04400957a6f8bb3f6610c",
        "deepnote_cell_type": "markdown",
        "id": "2v2D1coL7I8i"
      },
      "source": [
        "<h1><center>Laboratorio 6: La solicitud de Sergio ü§ó</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2024</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d3d6f6d405c54dbe985a5f4b3e4f9120",
        "deepnote_cell_type": "markdown",
        "id": "YxdTmIPD7L_x"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebasti√°n Tinoco\n",
        "- Auxiliar: Eduardo Moya\n",
        "- Ayudantes: Nicol√°s Ojeda, Melanie Pe√±a, Valentina Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "851a7788e8214942863cbd4099064ab2",
        "deepnote_cell_type": "markdown",
        "id": "Y2Gyrj-x7N2L"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Luis Pic√≥n\n",
        "- Nombre de alumno 2: Israel Astudillo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f23a189afdec4e198683308db70e43b7",
        "deepnote_cell_type": "markdown",
        "id": "jQ9skYc57Pxi"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/IsraPKMNPAP/Laboratorio-de-Herramientas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5318f41cda64d4290a7a548956ed725",
        "deepnote_cell_type": "markdown",
        "id": "1M4PoEWm7S80"
      },
      "source": [
        "## Temas a tratar\n",
        "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
        "- Aplicar Pipelines y Column Transformers.\n",
        "- Utilizar diferentes algoritmos de cluster y ver el desempe√±o.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Prohibidas las copias.\n",
        "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
        "- C√≥digo que no se pueda ejecutar, no ser√° revisado.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar clusters.\n",
        "- Familiarizarse con plotly.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "858df483d9e64780a21674afed1d34b8",
        "deepnote_cell_type": "markdown",
        "id": "SuMbiyQZG2Cc"
      },
      "source": [
        "## Descripci√≥n del laboratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "403ffe48ec994afda4b91e670a08d0ef",
        "deepnote_cell_type": "markdown",
        "id": "QZsNO4rUrqCz"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5a/a6/af/5aa6afde8490da403a21601adf7a7240.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0303baa17d4546feae8c9b88c58470bf",
        "deepnote_cell_type": "markdown",
        "id": "2o0MPuk8rqCz"
      },
      "source": [
        "En el coraz√≥n de las operaciones de Aerol√≠nea Lucero, Sergio, el gerente de an√°lisis de datos, reuni√≥ a un talentoso equipo de j√≥venes cient√≠ficos de datos para un desaf√≠o crucial: segmentar la base de datos de los clientes. ‚ÄúNuestro objetivo es descubrir patrones en el comportamiento de los pasajeros que nos permitan personalizar servicios y optimizar nuestras campa√±as de marketing,‚Äù explic√≥ Sergio, mientras desplegaba un amplio rango de datos que inclu√≠an desde h√°bitos de compra hasta opiniones sobre los vuelos.\n",
        "\n",
        "Sergio encarg√≥ a los cient√≠ficos de datos la tarea de aplicar t√©cnicas avanzadas de clustering para identificar distintos segmentos de clientes, como los viajeros frecuentes y aquellos que eligen la aerol√≠nea para celebrar ocasiones especiales. La meta principal era entender profundamente c√≥mo estos grupos perciben la calidad y satisfacci√≥n de los servicios ofrecidos por la aerol√≠nea.\n",
        "\n",
        "A trav√©s de un enfoque meticuloso y colaborativo, los cient√≠ficos de datos se abocaron a la tarea, buscando transformar los datos brutos en valiosos insights que permitir√≠an a Aerol√≠nea Lucero no solo mejorar su servicio, sino tambi√©n fortalecer las relaciones con sus clientes mediante una oferta m√°s personalizada y efectiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e78cb41b144041af98928ab26dcfdaa9",
        "deepnote_cell_type": "markdown",
        "id": "hs4KKWF1Hdpo"
      },
      "source": [
        "## Importamos librerias utiles üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "95a5533cfd6d49cfb9afc111c44d224f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 15,
        "execution_start": 1714107106552,
        "id": "a4YpMafirqC0",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "acbeab32db6146678e75448dddf43da8",
        "deepnote_cell_type": "markdown",
        "id": "UQOXod4gHhSq"
      },
      "source": [
        "## 1. Estudio de Performance üìà [10 Puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "704b56b978254ad3ae12cdbf58f4832d",
        "deepnote_cell_type": "markdown",
        "id": "Gn5u5ICkrqC2"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://user-images.githubusercontent.com/57133330/188281408-c67df9ee-fd1f-4b37-833b-f02848f1ce02.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d35fbdcc5ef045d6a2822622f0714179",
        "deepnote_cell_type": "markdown",
        "id": "y4Z0jTjtrqC2"
      },
      "source": [
        "Don Sergio les ha encomendado su primera tarea: analizar diversas t√©cnicas de clustering. Su objetivo es entender detalladamente c√≥mo funcionan estos m√©todos en t√©rminos de segmentaci√≥n y eficiencia en tiempo de ejecuci√≥n.\n",
        "\n",
        "Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering (k-means, DBSCAN, Ward y GMM) aplicados a tres conjuntos de datos, incrementando progresivamente su tama√±o. Utilice Plotly para las gr√°ficas y discuta los resultados tanto cualitativa como cuantitativamente.\n",
        "\n",
        "Uno de los requisitos establecidos por Sergio es que el an√°lisis se lleve a cabo utilizando Plotly; de no ser as√≠, se considerar√° incorrecto. Para facilitar este proceso, se ha proporcionado un c√≥digo de Plotly que puede servir como base para realizar las gr√°ficas. Ap√≥yese en el c√≥digo entregado para efectuar el an√°lisis y tome como referencia la siguiente imagen para realizar los gr√°ficos:\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-04-26_at_9.10.44_AM.png' width=800 />\n",
        "\n",
        "En el gr√°fico se visualizan en dos dimensiones los diferentes tipos de datos proporcionados en `datasets`. Cada columna corresponde a un modelo de clustering diferente, mientras que cada fila representa un conjunto de datos distinto. Cada uno de los gr√°ficos incluye el tiempo en segundos que tarda el an√°lisis y la m√©trica Silhouette obtenida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "37580aab6cef4238a8ce42c50a6d35de",
        "deepnote_cell_type": "markdown",
        "id": "maCUNAvZrqC2"
      },
      "source": [
        "Para ser m√°s espec√≠ficos, usted debe cumplir los siguientes objetivos:\n",
        "1. Generar una funci√≥n que permita replicar el gr√°fico expuesto en la imagen (no importa que los colores calcen). [4 puntos]\n",
        "2. Ejecuta la funci√≥n para un `n_samples` igual a 1000, 5000, 10000. [2 puntos]\n",
        "3. Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering utilizando las 3 configuraciones dadas en `n_samples`. [4 puntos]\n",
        "\n",
        "\n",
        "> ‚ùó Tiene libertad absoluta de escoger los hiper par√°metros de los cluster, sin embargo, se recomienda verificar el dominio de las variables para realizar la segmentaci√≥n.\n",
        "\n",
        "> ‚ùó Recuerde que es obligatorio el uso de plotly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cell_id": "7f7c25e366754595b13fc2e8116f65a0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 78,
        "execution_start": 1714107108441,
        "id": "i0IZPGPOrqC3",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "En la siguiente celda se crean los datos ficticios a usar en la secci√≥n 1 del lab.\n",
        "‚ùóNo realice cambios a esta celda a excepci√≥n de n_samples‚ùó\n",
        "\"\"\"\n",
        "\n",
        "# Datos a utilizar\n",
        "\n",
        "# Configuracion\n",
        "n_samples = 5000 #Este par√°metro si lo pueden modificar\n",
        "\n",
        "def create_data(n_samples):\n",
        "\n",
        "    # Lunas\n",
        "    moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=30)\n",
        "    # Blobs\n",
        "    blobs = datasets.make_blobs(n_samples=n_samples, random_state=172)\n",
        "    # Datos desiguales\n",
        "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "    mutated = (np.dot(blobs[0], transformation), blobs[1])\n",
        "\n",
        "    # Generamos Dataset\n",
        "    dataset = {\n",
        "        'moons':{\n",
        "            'x': moons[0], 'classes': moons[1], 'n_cluster': 2\n",
        "        },\n",
        "        'blobs':{\n",
        "            'x': blobs[0], 'classes': blobs[1], 'n_cluster': 3\n",
        "        },\n",
        "        'mutated':{\n",
        "            'x': mutated[0], 'classes': mutated[1], 'n_cluster': 3\n",
        "        }\n",
        "    }\n",
        "    return dataset\n",
        "\n",
        "data_sets = create_data(n_samples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y51s6f_UtIkc"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_id": "643d6b35af5541358f481fda4d3fc51f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 267,
        "execution_start": 1714108733824,
        "id": "CO3JFqezrqC3",
        "outputId": "61a8cca1-3460-44dc-c23f-7dffb853b9a6",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import time\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Definir la funci√≥n para crear un scatter plot para cada algoritmo\n",
        "def plot_scatter(x, y, color):\n",
        "  \n",
        "    # Juntamos las coordenadas x e y en una matriz\n",
        "    X = np.column_stack([x, y])\n",
        "\n",
        "    # Instancias de los algoritmos\n",
        "    kmeans = KMeans(n_clusters=3, random_state=35)\n",
        "    dbscan = DBSCAN(eps=0.35, min_samples=10)\n",
        "    ward = AgglomerativeClustering(n_clusters=3)\n",
        "    gmm = GaussianMixture(n_components=3, random_state=35)\n",
        "    \n",
        "    # KMeans \n",
        "    start_time = time.time()\n",
        "    kmeans_labels = kmeans.fit_predict(X)\n",
        "    kmeans_time = time.time() - start_time\n",
        "    silhouette_kmeans = silhouette_score(X, kmeans_labels)\n",
        "\n",
        "    # DBSCAN \n",
        "    start_time = time.time()\n",
        "    dbscan_labels = dbscan.fit_predict(X)\n",
        "    dbscan_time = time.time() - start_time\n",
        "    silhouette_dbscan = silhouette_score(X, dbscan_labels) if len(set(dbscan_labels)) > 1 else 0\n",
        "\n",
        "    # Ward\n",
        "    start_time = time.time()\n",
        "    ward_labels = ward.fit_predict(X)\n",
        "    ward_time = time.time() - start_time\n",
        "    silhouette_ward = silhouette_score(X, ward_labels)\n",
        "\n",
        "    # GMM\n",
        "    start_time = time.time()\n",
        "    gmm_labels = gmm.fit_predict(X)\n",
        "    gmm_time = time.time() - start_time\n",
        "    silhouette_gmm = silhouette_score(X, gmm_labels)\n",
        "\n",
        "    # Creamos una figura con subplots, una fila con tantas columnas como algoritmos\n",
        "    fig = make_subplots(rows=1, cols=4, subplot_titles=[f'KMeans\\n{round(kmeans_time, 2)} [s] | s: {round(silhouette_kmeans, 2)}',f'DBSCAN\\n{round(dbscan_time, 2)} [s] | s: {round(silhouette_dbscan, 2)}',f'Ward\\n{round(ward_time, 2)} [s] | s: {round(silhouette_ward, 2)}',f'GMM\\n{round(gmm_time, 2)} [s] | s: {round(silhouette_gmm, 2)}'])\n",
        "    \n",
        "    # Scatter plot para KMeans\n",
        "    fig.add_trace(go.Scatter(x=x, y=y, mode='markers', marker=dict(color=kmeans_labels, colorscale='Viridis')),row=1, col=1) \n",
        "\n",
        "    #Decidimos usar color = algoritmo_labels y no color = color a modo de que los clusters observados a la hora de graficar sean m√°s parecidos a los de la im√°gen del lab.\n",
        "\n",
        "    # Scatter plot para GMM \n",
        "    fig.add_trace(go.Scatter(x=x, y=y, mode='markers', marker=dict(color=gmm_labels, colorscale='Viridis')),row=1, col=4)\n",
        "    \n",
        "    # Scatter plot para Ward\n",
        "    fig.add_trace(go.Scatter(x=x, y=y, mode='markers', marker=dict(color=ward_labels, colorscale='Viridis')),row=1, col=3)\n",
        "    \n",
        "    # Scatter plot para DBSCAN\n",
        "    fig.add_trace(go.Scatter(x=x, y=y, mode='markers', marker=dict(color=dbscan_labels, colorscale= 'Viridis')),row=1, col=2)\n",
        "    \n",
        "    fig.update_layout(title_text=\"Comparaci√≥n de tiempos de ejecuci√≥n por t√©cnica\", height=400)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graficamos\n",
        "fig1 = plot_scatter(data_sets['moons']['x'][:, 0], data_sets['moons']['x'][:, 1], data_sets['moons']['classes'])\n",
        "fig2 = plot_scatter(data_sets['blobs']['x'][:, 0], data_sets['blobs']['x'][:, 1], data_sets['blobs']['classes'])\n",
        "fig3 = plot_scatter(data_sets['mutated']['x'][:, 0], data_sets['mutated']['x'][:, 1], data_sets['mutated']['classes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4**\n",
        "\n",
        "Con repecto a las visualizaciones, lo primero que notamos es que al disminuir el numero de n_samples, la densidad de las gr√°ficas es menor obviamente porque estamos visualizando menos datos,adem√°s, logramos observar (con n_sampes = 1000) algunos mini clusters adicionales en la grafica para el segundo conjunto de datos con respecto a otros valores m√°s grandes de n_samples, esto probablemente es causado por la configuraci√≥n de los hiperpar√°metros del algoritmo, ya que DBSCAN, al trabajar sobre un conjunto de datos m√°s peque√±o,donde las distancias entre puntos suelen ser menores en t√©rminos relativos, encontrar√° m√°s puntos que est√©n dentro del radio definido por los hiperpar√°metros (eps). Otro aspecto importante es que al cambiar la variable n_samples se forman distintos clusters para los mismos algoritmos aplicados a los mismos conjuntos de datos.\n",
        "Por otro lado, con respecto al rendimiento de los algoritmos y tiempos de ejecuci√≥n notamos que a medida que aumenta n_samples, obviamente el tiempo de ejecuci√≥n es mayor, sin embargo en la mayor√≠a de los casos el rendimiento (coeficiente de Silohuette) se mantiene constante. En el caso del agoritmo Ward (AgglomerativeClustering) aplicado al conjunto de datos moons, el rendimiento empeora cuando n_samples aumenta y para los casos de el algoritmo DBSCAN aplicado a los conjuntos de datos blobs y mutated el rendimiento aumenta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "13c5cb8067d9415f83b3d497954a437a",
        "deepnote_cell_type": "markdown",
        "id": "3mCbZc86rqC6"
      },
      "source": [
        "## 2. An√°lisis de Satisfacci√≥n de Vuelos. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd6e991646b44f50a4b13f01d1542415",
        "deepnote_cell_type": "markdown",
        "id": "JI33m5jbrqC6"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.gifer.com/2Hci.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5742dfbd5a2e43778ff250436bab1005",
        "deepnote_cell_type": "markdown",
        "id": "h5k24znirqC7"
      },
      "source": [
        "Habiendo entendido c√≥mo funcionan los modelos de aprendizaje no supervisado, *Don Sergio* le encomienda estudiar la satisfacci√≥n de pasajeros al haber tomado un vuelo en alguna de sus aerolineas. Para esto, el magnate le dispone del dataset `aerolineas_licer.parquet`, el cual contiene el grado de satisfacci√≥n de los clientes frente a diferentes aspectos del vuelo. Las caracter√≠sticas del vuelo se definen a continuaci√≥n:\n",
        "\n",
        "- *Gender*: G√©nero de los pasajeros (Femenino, Masculino)\n",
        "- *Customer Type*: Tipo de cliente (Cliente habitual, cliente no habitual)\n",
        "- *Age*: Edad actual de los pasajeros\n",
        "- *Type of Travel*: Prop√≥sito del vuelo de los pasajeros (Viaje personal, Viaje de negocios)\n",
        "- *Class*: Clase de viaje en el avi√≥n de los pasajeros (Business, Eco, Eco Plus)\n",
        "- *Flight distance*: Distancia del vuelo de este viaje\n",
        "- *Inflight wifi service*: Nivel de satisfacci√≥n del servicio de wifi durante el vuelo (0:No Aplicable; 1-5)\n",
        "- *Departure/Arrival time convenient*: Nivel de satisfacci√≥n con la conveniencia del horario de salida/llegada\n",
        "- *Ease of Online booking*: Nivel de satisfacci√≥n con la facilidad de reserva en l√≠nea\n",
        "- *Gate location*: Nivel de satisfacci√≥n con la ubicaci√≥n de la puerta\n",
        "- *Food and drink*: Nivel de satisfacci√≥n con la comida y la bebida\n",
        "- *Online boarding*: Nivel de satisfacci√≥n con el embarque en l√≠nea\n",
        "- *Seat comfort*: Nivel de satisfacci√≥n con la comodidad del asiento\n",
        "- *Inflight entertainment*: Nivel de satisfacci√≥n con el entretenimiento durante el vuelo\n",
        "- *On-board service*: Nivel de satisfacci√≥n con el servicio a bordo\n",
        "- *Leg room service*: Nivel de satisfacci√≥n con el espacio para las piernas\n",
        "- *Baggage handling*: Nivel de satisfacci√≥n con el manejo del equipaje\n",
        "- *Check-in service*: Nivel de satisfacci√≥n con el servicio de check-in\n",
        "- *Inflight service*: Nivel de satisfacci√≥n con el servicio durante el vuelo\n",
        "- *Cleanliness*: Nivel de satisfacci√≥n con la limpieza\n",
        "- *Departure Delay in Minutes*: Minutos de retraso en la salida\n",
        "- *Arrival Delay in Minutes*: Minutos de retraso en la llegada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoIFHpw5xCW"
      },
      "source": [
        "En consideraci√≥n de lo anterior, realice las siguientes tareas:\n",
        "\n",
        "0. Ingeste el dataset a su ambiente de trabajo.\n",
        "\n",
        "1. Seleccione **s√≥lo las variables num√©ricas del dataset**.  Explique qu√© √©fectos podr√≠a causar el uso de variables categ√≥ricas en un algoritmo no supervisado. [2 punto]\n",
        "\n",
        "2. Realice una visualizaci√≥n de la distribuci√≥n de cada variable y analice cada una de estas distribuciones. [2 punto]\n",
        "\n",
        "3. Bas√°ndose en los gr√°ficos, eval√∫e la necesidad de escalar los datos y explique el motivo de su decisi√≥n. [2 puntos]\n",
        "\n",
        "4. Examine la correlaci√≥n entre las variables mediante un correlograma. [2 puntos]\n",
        "\n",
        "5. De acuerdo con los resultados obtenidos en 5, reduzca la dimensionalidad del conjunto de datos a cuatro variables, justificando su elecci√≥n respecto a las variables que decide eliminar. [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6tcVBCtxxS"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pzHTZ17xveU_"
      },
      "outputs": [],
      "source": [
        "# Carga de datos\n",
        "df = pd.read_parquet('../temp/aerolineas_lucer.parquet')\n",
        "\n",
        "# Seleccionar solo las variables num√©ricas\n",
        "df_numericas = df.select_dtypes(include=['int64', 'float64'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El uso de variables categ√≥ricas en este tipo de algoritmos puede generar resultados err√≥neos e interpretaciones err√≥neas debido a que, por ejemplo, los algortimos de clusterizaci√≥n calculan distancias entre los datos para realizar los agrupamientos, cosa que no es posible de realizar con variables categ√≥ricas, obviamente debido a que no poseen una relaci√≥n matem√°tica natural que pueda reflejarse en distancias.\n",
        "Sin embargo, esto se puede solucionar utilizando otros algoritmos, como k-modes, que est√©n mejor adaptados para manejar datos categ√≥ricos y que utilicen m√©tricas de disimilitud adecuadas para este tipo de informaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graficamos los histogramas de todas las variables num√©ricas\n",
        "fig = df_numericas.hist(bins=70, figsize=(15, 17))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graficamos la variable Departure Delay in Minutes por separado para analizarla en m√°s detalle\n",
        "hist_delay = px.histogram(\n",
        "    df_numericas,\n",
        "    x=\"Departure Delay in Minutes\",\n",
        "    nbins=150,\n",
        "    title=\"Histograma de la variable Departure Delay in Minutes\",\n",
        "    labels={'x': 'Delay in Minutes', 'y': 'Frecuencia'},\n",
        "    template='presentation'\n",
        ")\n",
        "\n",
        "# Personalizar los ejes\n",
        "hist_delay.update_xaxes(title_text=\"Departure Delay in Minutes\", showgrid=True, gridwidth=1, gridcolor='gray')\n",
        "hist_delay.update_yaxes(title_text=\"Frecuencia\", showgrid=True, gridwidth=1, gridcolor='gray')\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "hist_delay.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2**\n",
        "\n",
        "Con respecto a la variable edad podemos notar su distribuci√≥n es parecida a una distribuci√≥n normal, si no fuera por algunas edades que poseen una frecuencia muy grande y se 'escapan' de esta distribuci√≥n.\n",
        "Los datos asociados a la distancia de vuelo, muestran que la mayoria de los vuelos son de distancias cortas (0km - 1000km), luego, a medida que la distancia va creciendo, la frecuencia de vuelos disminuye hasta mantenerse relativamente constante entre los (2500km - 4000km).\n",
        "Con respecto a las variables inflight wifi service, ease of online booking, gate location, checkin service, podemos decir que tienen una calificaci√≥n promedio regular (3 en una escala de 0 a 5).\n",
        "De la misma forma, podemos decir que las variables Departure/Arrive time convenient, Food and drink, Online boarding, Seat comfort, Inflight entertainment, On-board service, Leg room service, Baggage handling, Inflight service, Cleanliness, poseen una mejor calificaci√≥n en promedio con respecto a las variables mencionadas anteriormente, ya que la mayor√≠a de los datos se encuentran concentrados a la derecha (asociados a una mejor calificaci√≥n).\n",
        "Finalmente podemos notar que las variables Departure Delay in Minutes y Arrival Delay in minutes tienen una distribuci√≥n muy similar, lo cual es esperable debido a que un vuelo retrasado significa un vuelo que llega tarde a su destino. Al ver la distribuci√≥n se puede ver que la mayor√≠a de los vuelos (190.428) poseen retrasos de 0 a 20 minutos, luego, la cantidad de vuelos que tienen un retraso mayor a este disminuye r√°pidamente a aproximadamente 20 mil vuelos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3**\n",
        "\n",
        "A partir de los gr√°ficos de las distribuciones podemos concluir que si es necesario escalar los datos, esto porque las variables tienen diferentes escalas y rangos, como la edad y la distancia de vuelo, lo que podr√≠a afectar a aquellos alogoritmos, como K-means, que calculan distancias entre los datos. Adem√°s, algunas variables, como los retrasos en la salida y llegada, tienen distribuciones con valores extremos, adem√°s, las calificaciones de satisfacci√≥n, aunque est√°n en una escala limitada, pueden homogeneizarse para evitar que ciertas variables tengan m√°s peso en el an√°lisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculamos la matriz de correlaci√≥n\n",
        "correlation_matrix = df_numericas.corr()\n",
        "\n",
        "# Graficamos la matriz de correlaci√≥n\n",
        "fig = px.imshow(correlation_matrix, text_auto=True, aspect=\"auto\", color_continuous_scale='Viridis')\n",
        "\n",
        "# Ajustamos el gr√°fico\n",
        "fig.update_layout(width=1100,height=1000,title_text=\"Mapa de Correlaci√≥n de Variables Num√©ricas\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que las correlaciones m√°s importantes son:\n",
        "- Cleanliness con Food and drink, Seat comfort e Inflight entertainment\n",
        "- Inflight service con Baggage handling y On-board service\n",
        "- Food and drink con Inflight entertainment y Seat comfort\n",
        "- Inflight wifi service con Ease of online booking\n",
        "- Departure Delay con Arrival Delay\n",
        "- Inflight entertainment es la variable que con m√°s variables se correlaciona (sobre 0.2)\n",
        "\n",
        "De lo anterior podemos decir que nos aporta quedarnos con:\n",
        "Edad y distancia de vuelo dado que no hay otras variables que las expliquen debido a su baja correlaci√≥n con el resto de variables y aportan informaci√≥n √∫nica por lo tanto.\n",
        "Departure Delay contando por las variables de retraso que se explican entre s√≠ y no hay otras variables que las expliquen, por lo que incluir una aporta informaci√≥n.\n",
        "Finalmente, la variable representativa de las variables de calidad de viaje es la variable Inflight entertainment dado que puede explicar la mayro cantidad de variables por su covarianza relativamente m√°s alta con el resto de variables de este tipo, y menor correlaci√≥n con las variables ya escogidas.\n",
        "Quedando finalmente:\n",
        "Age, Flight Distance, Inflight entertainment y Departure Delay in Minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reducci√≥n de dimensionalidad a 4 variables\n",
        "df_redux = df_numericas[[\"Age\",\"Flight Distance\",\"Inflight entertainment\",\"Departure Delay in Minutes\"]]\n",
        "df_redux.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4b6c047d994f40ea9e78a36a777042e0",
        "deepnote_cell_type": "markdown",
        "id": "PNGfTgtkrqC9"
      },
      "source": [
        "## 3. Preprocesamiento üé≠. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "713b3f0e61dd4841bb5b38c730d344d5",
        "deepnote_cell_type": "markdown",
        "id": "6RZD0fMNrqC-"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/1e/a8/0e/1ea80e7cea0d429146580c7e91c5b944.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "98400c7b5fec4af193eec3601f53891e",
        "deepnote_cell_type": "markdown",
        "id": "J6d4VEOTrqC-"
      },
      "source": [
        "Tras quedar satisfecho con los resultados presentados en el punto 2, el due√±o de la empresa ha solicitado que se preprocesen los datos mediante un `pipeline`. Es crucial que este proceso tenga en cuenta las observaciones derivadas de los an√°lisis anteriores. Adicionalmente, ha expresado su inter√©s en visualizar el conjunto de datos en un gr√°fico de dos o tres dimensiones.\n",
        "\n",
        "Bas√°ndose en los an√°lisis realizados anteriormente:\n",
        "1. Cree un `pipeline` que incluya PCA, utilizando las consideraciones mencionadas previamente para proyectar los datos a dos dimensiones. [4 puntos]\n",
        "2. Grafique los resultados obtenidos y comente lo visualizado. [6 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDSaGoq0OUp"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "ad1e70818ad748638ca0927b07a76125",
        "deepnote_cell_type": "code",
        "id": "gBYG238wrqC-"
      },
      "outputs": [],
      "source": [
        "# Escriba su c√≥digo aqu√≠\n",
        "\n",
        "# Paquetes\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Inicializamos PCA\n",
        "pca = PCA(n_components=2)\n",
        "#X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Inicializamos escalado\n",
        "standard_scaler = StandardScaler()\n",
        "\n",
        "# Pipeline\n",
        "pipe_pca = Pipeline([\n",
        "    (\"Scaler\",standard_scaler),\n",
        "    (\"PCA\",pca)\n",
        "])\n",
        "\n",
        "pca_transf = pipe_pca.fit_transform(df_redux)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataframe para graficar\n",
        "df_pca = pd.DataFrame(pca_transf, columns=['PC1', 'PC2'])\n",
        "\n",
        "fig = px.scatter(df_pca, x='PC1', y='PC2', title='Clusters visualizados en espacio PCA, datos escalados',\n",
        "                 labels={'PC1': 'Componente Principal 1', 'PC2': 'Componente Principal 2'})\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notamos que al pasar los datos escalados y reducidos a un espacio de dimensi√≥n 2, la mayor√≠a de los datos transformados quedan aglomerados en un espacio en la parte baja del gr√°fico, mientras que otro conjunto de datos (aunque la minor√≠a) se observan alejados de esta aglomeraci√≥n ,lo cual es m√°s evidente de ver al avanzar en el eje y del gr√°fico. Luego, podr√≠amos decir que estos datos podr√≠an ser categorizados como outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bd281470d3054764a63d857cfa7d52a6",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "7ENoOtIIrqC_"
      },
      "source": [
        "## 4. Outliers üö´üôÖ‚Äç‚ôÄÔ∏è‚ùåüôÖ‚Äç‚ôÇÔ∏è [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db89e9c9f35c44abbd8991180226c0ea",
        "deepnote_cell_type": "markdown",
        "id": "fbGw6Sa-rqC_"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://joachim-gassen.github.io/images/ani_sim_bad_leverage.gif\" width=250>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3e2f59fa12954641af7a854a4e203694",
        "deepnote_cell_type": "markdown",
        "id": "nl_ccu9brqDA"
      },
      "source": [
        "Con el objetivo de mantener la claridad en su an√°lisis, Don Sergio le ha solicitado entrenar un modelo que identifique pasajeros con comportamientos altamente at√≠picos.\n",
        "\n",
        "1. Utilice `IsolationForest` para clasificar las anomal√≠as del dataset (sin aplicar PCA), configurando el modelo para que s√≥lo el 1% de los datos sean considerados an√≥malos. Aseg√∫rese de integrar esta tarea dentro de un `pipeline`. [3 puntos]\n",
        "\n",
        "2. Visualice los resultados en el gr√°fico de dos dimensiones previamente creado. [3 puntos]\n",
        "\n",
        "3. ¬øC√≥mo evaluar√≠a el rendimiento de su modelo en la detecci√≥n de anomal√≠as? [4 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de responder fijamos librer√≠as y pipelines √∫tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paquetes\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Inicializamos PCA\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Inicializamos escalado\n",
        "standard_scaler = StandardScaler()\n",
        "\n",
        "# Pipeline\n",
        "pipe_pca = Pipeline([\n",
        "    (\"Scaler\",standard_scaler),\n",
        "    (\"PCA\",pca)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reducci√≥n de dimensionalidad a 4 variables\n",
        "df_redux = df_numericas[[\"Age\",\"Flight Distance\",\"Inflight entertainment\",\"Departure Delay in Minutes\"]]\n",
        "df_redux.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cS1FR00NlF"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "be86896911244aa89e3b5f3f00a286af",
        "deepnote_cell_type": "code",
        "id": "iaPZFmjyrqDA"
      },
      "outputs": [],
      "source": [
        "# Escriba su c√≥digo aqu√≠\n",
        "\n",
        "# Paquetes\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "\n",
        "# Isolation Forest\n",
        "# Recordamos que no requiere normalizaci√≥n\n",
        "isf = IsolationForest(n_estimators=20, random_state=30, contamination=0.01)\n",
        "\n",
        "# Pipeline nuevo.\n",
        "isf_pipeline = Pipeline([\n",
        "    ('ISF', isf)\n",
        "    ])\n",
        "\n",
        "# Vemos el resultado\n",
        "outliers = isf_pipeline.fit_predict(df_redux)\n",
        "\n",
        "# Conteo de Outliers\n",
        "unique_values, count = np.unique(outliers, return_counts=True)\n",
        "for valor, conteo in zip(unique_values, count):\n",
        "    print(f'Valor: {valor}, Conteo: {conteo}')\n",
        "print(\"-------------------------------------\")\n",
        "# Efectivamente el 1% fue marcado como outlier\n",
        "print(\"Total de datos: \"+str(len(df_redux))+\"\\n\"+\n",
        "      \"Un porciento del total de datos: \"+str(int(len(df_redux)*0.01)))\n",
        "print(\"-------------------------------------\")\n",
        "# Generamos el dataframe filtrado\n",
        "mask = outliers>0\n",
        "df_isf = df_redux[mask]\n",
        "\n",
        "# Vemos lo obtenido\n",
        "print(\"Filas antes de filtrar: \"+str(df_redux.shape[0])+\"\\n\"+\n",
        "      \"Filas despu√©s de filtrar: \"+str(df_isf.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gr√°fico de resultados PCA\n",
        "\n",
        "# Comparamos visualizaciones\n",
        "\n",
        "# Dataframe de datos sin outliers y sin escalar\n",
        "pca_2 = pca.fit_transform(df_isf)\n",
        "df_pca_2 = pd.DataFrame(pca_2,columns=['PC1', 'PC2'])\n",
        "\n",
        "# Dataframe de datos sin outliers y escalados\n",
        "pca_outs = pipe_pca.fit_transform(df_isf)\n",
        "df_pca_outs = pd.DataFrame(pca_outs, columns=['PC1', 'PC2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.scatter(df_pca_2, x='PC1', y='PC2', title='PCA sin escalamiento ni outliers',\n",
        "                 labels={'PC1': 'Componente Principal 1', 'PC2': 'Componente Principal 2'})\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.scatter(df_pca_outs, x='PC1', y='PC2', title='PCA con escalamiento y sin outliers',\n",
        "                 labels={'PC1': 'Componente Principal 1', 'PC2': 'Componente Principal 2'})\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preferimos la visualizaci√≥n escalada que muestra un continuo regular del conjunto de datos para visualizar en el espacio PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es posible evaluar el desempe√±o de la clasificaci√≥n de anomal√≠as compar√°ndolo con los resultados de utilizar el rango intercuart√≠lico para clasificar outliers univariadamente por columna, sin embargo esta comparaci√≥n es limitada dado que este m√©todo asume normalidad en la distribcui√≥n de la columna con outliers, no determina una cantidad fija de outliers y no toma en cuenta posibles outliers multivariados. De todas maneras puede ser √∫til comparar el resultado de Isolation Forest si es que la columna tiene una distribcui√≥n similar a la normal, para evaluar que tan v√°lidos son los supuestos de aislaci√≥n de outlayers de este m√©todo.\n",
        "Si los datos no distribuyen normal podr√≠a ser √∫til hacer clustering con DBSCAN para evaluar cuales son los puntos considerados outliers por el algoritmo y analizar los distintos par√°metros de los modelos que ofrecen respuestas similares, y que implica esta decisi√≥n sobre los datos, es decir si muchos datos son clasificados outliers o si la clasificaci√≥n va en contra del conocimiento a priori quiz√° no es la aproximaci√≥n deseada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3871e2fe5bdd422dbdbfaebf75503ae3",
        "deepnote_cell_type": "markdown",
        "id": "zQFTklmVrqDB"
      },
      "source": [
        "## 5. M√©tricas de Desempe√±o üöÄ [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "236333de6dd445c182aefcc507589325",
        "deepnote_cell_type": "markdown",
        "id": "YpNj4wbPrqDB"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://giffiles.alphacoders.com/219/219081.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a7e1ceb91be94b1da2ab8be97dfac999",
        "deepnote_cell_type": "markdown",
        "id": "CR3hzRxrrqDB"
      },
      "source": [
        "Motivado por incrementar su fortuna, Don Sergio le solicita entrenar un modelo que le permita segmentar a los pasajeros en grupos distintos, con el objetivo de optimizar las diversas campa√±as de marketing dise√±adas por su equipo. Para ello, le se pide realizar las siguientes tareas:\n",
        "\n",
        "1. Utilizar el modelo **Gaussian Mixture** y explore diferentes configuraciones de n√∫mero de cl√∫sters, espec√≠ficamente entre 3 y 8. Aseg√∫rese de integrar esta operaci√≥n dentro de un `pipeline`. [4 puntos]\n",
        "2. Explique cu√°l ser√≠a el criterio adecuado para seleccionar el n√∫mero √≥ptimo de cl√∫sters. **Justifique de forma estadistica y a traves de gr√°ficos.** [6 puntos]\n",
        "\n",
        "> **HINT:** Se recomienda investigar sobre los criterios AIC y BIC para esta tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_T_zTg0MXB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dos set de datos a comparar\n",
        "# Ambos escalados, dado que facilita la convergencia de gmm\n",
        "# Datos sin outliers y escalados\n",
        "df_isf_scaled = standard_scaler.fit_transform(df_isf) # Escalamos los datos sin outliers\n",
        "# Datos sin outliers, escalados y en el espacio pca\n",
        "#df_pca_outs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cell_id": "6d3d1bb3fda14321984466d9101a775a",
        "deepnote_cell_type": "code",
        "id": "5GeUb9J3rqDB"
      },
      "outputs": [],
      "source": [
        "# Escriba su c√≥digo aqu√≠\n",
        "\n",
        "# Iteraci√≥n sobre los datos SCALED + NO OUTLIERS en listas sin _\n",
        "# Iteraci√≥n sobre datos con PCA + SCALER + NO OUTLIERS en listas con _\n",
        "\n",
        "gmm_iter = []\n",
        "aic = []\n",
        "bic = []\n",
        "\n",
        "gmm_iter_ = []\n",
        "aic_ = []\n",
        "bic_ = []\n",
        "\n",
        "for i in range(3,9):\n",
        "    gmm = GaussianMixture(n_components = i, random_state=30)\n",
        "\n",
        "    pipe = Pipeline([\n",
        "    (\"Scaler\", standard_scaler),\n",
        "    (\"gmm\", gmm)\n",
        "    ])\n",
        "\n",
        "    clusters = pipe.fit_predict(df_isf_scaled)\n",
        "    gmm_iter.append(clusters)\n",
        "    \n",
        "    # M√©tricas sin outliers escalado\n",
        "    aic.append(pipe.named_steps['gmm'].aic(pipe.named_steps['Scaler'].transform(df_isf_scaled)))\n",
        "    bic.append(pipe.named_steps['gmm'].bic(pipe.named_steps['Scaler'].transform(df_isf_scaled)))\n",
        "\n",
        "    gmm_pca = GaussianMixture(n_components=i, random_state=30)\n",
        "    gmm_pca.fit(df_pca_outs)\n",
        "    clusters_ = gmm_pca.predict(df_pca_outs)\n",
        "    gmm_iter_.append(clusters_)\n",
        "\n",
        "    # M√©tricas sin outliers, escalado y con transformaci√≥n PCA\n",
        "    aic_.append(gmm_pca.aic(df_pca_outs))\n",
        "    bic_.append(gmm_pca.bic(df_pca_outs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear una figura con dos subplots, uno al lado del otro (fila 1, columnas 2)\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"AIC y BIC\", \"AIC y BIC sobre el espacio PCA\"))\n",
        "\n",
        "range_ = range(3, 9)\n",
        "\n",
        "# Primer gr√°fico - AIC y BIC\n",
        "fig.add_trace(go.Scatter(x=list(range_), y=aic,\n",
        "                         mode='lines+markers',\n",
        "                         name='AIC',\n",
        "                         line=dict(color='blue', width=2)),\n",
        "              row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=list(range_), y=bic,\n",
        "                         mode='lines+markers',\n",
        "                         name='BIC',\n",
        "                         line=dict(color='red', width=2)),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Segundo gr√°fico - AIC_ y BIC_ (sobre espacio PCA)\n",
        "fig.add_trace(go.Scatter(x=list(range_), y=aic_,\n",
        "                         mode='lines+markers',\n",
        "                         name='AIC (PCA)',\n",
        "                         line=dict(color='blue', width=2)),\n",
        "              row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=list(range_), y=bic_,\n",
        "                         mode='lines+markers',\n",
        "                         name='BIC (PCA)',\n",
        "                         line=dict(color='red', width=2)),\n",
        "              row=1, col=2)\n",
        "\n",
        "# Actualizar el dise√±o de la figura\n",
        "fig.update_layout(title_text=\"Comparaci√≥n de AIC y BIC en los dos espacios\",\n",
        "                  xaxis_title='N√∫mero de clusters',\n",
        "                  yaxis_title='Criterio de informaci√≥n',\n",
        "                  template='plotly_dark')\n",
        "\n",
        "# Actualizar etiquetas de los ejes para cada gr√°fico\n",
        "fig.update_xaxes(title_text=\"N√∫mero de clusters\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Criterio de informaci√≥n\", row=1, col=1)\n",
        "\n",
        "fig.update_xaxes(title_text=\"N√∫mero de clusters\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Criterio de informaci√≥n\", row=1, col=2)\n",
        "\n",
        "# Mostrar el gr√°fico con los dos subplots\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que el n√∫mero de clusters con menor p√©rdida de informaci√≥n AIC y BIC es 7, sirviendo de \"codo\" referenciando el m√©todo del codo de selecci√≥n de clusters. Se ve que la p√©rdida de informaci√≥n decae a partir de este n√∫mero de clusters. Comparativamente los criterios tienen casi exactamente el mismo valor para ambas m√©tricas, a√∫n cuando se considera que BIC es m√°s estricta dado que penaliza por complejidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta iteraci√≥n vemos que ambos criterios de informaci√≥n disminuyen hasta 8 y se observa codos en 5 y 7 con descensos m√°s dr√°sticos. Nuevamente ambos criterios son muy similares entre s√≠ y no difieren sustancialmente. Dado que el dataset sin la transformaci√≥n pca incluye mayor cantidad de informaci√≥n dada la p√©rdida que ocurre al realizar la transformaci√≥n pca, decidimos elegir 7 como el n√∫mero de clusters apropiados, y en general tambi√©n se tiene que los criterios tienen un menor valor general en la iteraci√≥n sin pca."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sobre el dataset escalado, sin outliers y aplicado pca agregamos los clusters que encontramos sobre la data sin pca.\n",
        "df_pca_outs.loc[:,\"Clusters\"] = gmm_iter[7-3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "dd342e336254418ba766b29dce16b267",
        "deepnote_cell_type": "markdown",
        "id": "P9CERnaerqDC"
      },
      "source": [
        "## 6. An√°lisis de resultados üìä [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "953b5ad01a704b50b899db7176d1b7b2",
        "deepnote_cell_type": "markdown",
        "id": "I1yNa111rqDC"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.gifer.com/7wTk.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd90e2f135404353ac0b5ab844936ca7",
        "deepnote_cell_type": "markdown",
        "id": "dg0Qx4RZrqDC"
      },
      "source": [
        "Una vez identificado el n√∫mero √≥ptimo de cl√∫sters, se le pide realizar lo siguiente:\n",
        "\n",
        "1. Utilizar la proyecci√≥n en dos dimensiones para visualizar cada cl√∫ster claramente. [2 puntos]\n",
        "\n",
        "2. ¬øEs posible distinguir claramente entre los cl√∫sters generados? [2 puntos]\n",
        "\n",
        "3. Proporcionar una descripci√≥n breve de cada cl√∫ster utilizando estad√≠sticas descriptivas b√°sicas, como la media y la desviaci√≥n est√°ndar, para resumir las caracter√≠sticas de las variables utilizadas en estos algoritmos. [2 puntos]\n",
        "\n",
        "4. Proceda a visualizar los cl√∫sters en tres dimensiones para una perspectiva m√°s detallada. [2 puntos]\n",
        "\n",
        "5. ¬øC√≥mo afecta esto a sus conclusiones anteriores? [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRN0zZip0IMB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "9abf4dbc643e40cebe99fcb1ff3ff413",
        "deepnote_cell_type": "code",
        "id": "XmZrz15GrqDC"
      },
      "outputs": [],
      "source": [
        "# Escriba su c√≥digo aqu√≠\n",
        "# Gr√°fico de resultados PCA\n",
        "\n",
        "fig = px.scatter(df_pca_outs, x='PC1', y='PC2', color='Clusters', title='Clusters visualizados en espacio PCA',\n",
        "                 labels={'PC1': 'Componente Principal 1', 'PC2': 'Componente Principal 2'},\n",
        "                 color_continuous_scale=px.colors.sequential.Viridis)\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se visualiza que los clusters se superponen y es algo dif√≠cil distinguir los clusters entre s√≠, por lo que se dice que no son f√°cilmente identificables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creamos un dataframe nuevo desde que eliminamos outliers con los clusters actuales, sin estandarizar\n",
        "df_ = df_isf.copy()\n",
        "df_.loc[:,\"Clusters\"] = gmm_iter[7-3]\n",
        "\n",
        "# Agrupamos\n",
        "grouped = df_.groupby(\"Clusters\")\n",
        "\n",
        "# Media\n",
        "means = grouped.mean()\n",
        "\n",
        "# Desv Est√°ndar\n",
        "stds = grouped.std()\n",
        "\n",
        "# Mostramos\n",
        "print(means)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(stds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los clusters son similares entre s√≠ para la variable edad, no parecen diferenciarse en esta variable dado que sus medias y desviaciones son similares. Por otro lado, la variable de distancia de vuelo genera distinciones mas notorias, en espec√≠fico el cluster 3 tiene una distancia de vuelo media muy grande. El resto de clusters parece separarse en viajes cortos de media cercana a 500 (4 y 7), viajes de mediana extensi√≥n cercanos al 1000 promedio (5 y 6) y viajes largos con media sobre 1500 (1,2 y 3). Cabe mencionar que los clusters 1 y 2 tienen una media y desviaci√≥n est√°ndar alta y similar entre s√≠ para la variable de distancia de viaje. Para estos clusters nos fijamos que lo que realmente los diferencia es la desviaci√≥n est√°ndar de su score de entretenimiento y la media y desviaci√≥n est√°ndar del retraso de salida. Es decir estos dos cluster representan viajes de distancias similares altas que tuvieron o no retraso en su salida.\n",
        "Otra cosa notable es que casi todos los coeficientes de entretenimiento var√≠an entre 3 y 4, salvo el del cluster 1 ya caracterizado por viajes largos y poco tiempo de retraso en salida.\n",
        "Finalmente es notable que hay 3 clusters, el 3, 6 y 7, que tienen media y desviaci√≥n est√°ndar 0 en sus retrasos. Es decir los viajes con m√≠nimo retraso de salida est√°n en estos clusters, que se diferencian en su distancia de vuelo principalmente, de acuerdo a la distinci√≥n de viaje corto, medio y largo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataframe normalizado con las etiquetas de los clusters sin haber aplicado pca dos dimensiones, df_isf_scaled.\n",
        "\n",
        "pca_3d = PCA(n_components=3)\n",
        "\n",
        "pca_transf_3d = pca_3d.fit_transform(df_isf_scaled)\n",
        "\n",
        "df_3d = pd.DataFrame(pca_transf_3d, columns=['PC1', 'PC2','PC3'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gr√°fico\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=df_3d['PC1'],\n",
        "    y=df_3d['PC2'],\n",
        "    z=df_3d['PC3'],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=5,\n",
        "        color=df_isf_scaled['Clusters'],  # Si tienes etiquetas de cluster, las usas para colorear\n",
        "        colorscale='Viridis',  # Escala de colores\n",
        "        opacity=0.8\n",
        "    )\n",
        ")])\n",
        "\n",
        "# Configurar el dise√±o del gr√°fico\n",
        "fig.update_layout(\n",
        "    title='Gr√°fico 3D de PCA',\n",
        "    scene=dict(\n",
        "        xaxis_title='PC1',\n",
        "        yaxis_title='PC2',\n",
        "        zaxis_title='PC3'\n",
        "    ),\n",
        "    template='plotly_dark'  # Opcional: tema oscuro\n",
        ")\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si vemos el gr√°fico verticalmente en PC1 vemos que es posible diferenciar los clusters encontrados mucho m√°s f√°cilmente en el espacio, salvo por los clusters 5 y 6 que siguen estando bastante cercanos en el espacio. Sin emabrgo estos clusters si son distintos, dado que sus medias en el largo del viaje y retraso de salida son distintos entre grupos. Sin emabrgo se puede visualizar mucho m√°s claramente los clusters y efectivamente se logran visualizar, cambiando la conclusi√≥n de PCA en dos dimensiones."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "7cb425aec99b4079954fd707109c42c3",
    "deepnote_persisted_session": {
      "createdAt": "2024-04-26T06:15:51.197Z"
    },
    "kernelspec": {
      "display_name": "lab_progr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
